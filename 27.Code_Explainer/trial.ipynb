{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c58b69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter,Language\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a83b12c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d8563",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"/Users/yogeshagrawal/Desktop/Gen AI/27.Code_Explainer/code_repo\"\n",
    "# Repo.clone_from(\"https://github.com/entbappy/End-to-end-Medical-Chatbot-Generative-AI\", to_path = repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b242418f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yogeshagrawal/Desktop/Gen AI/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 7/7 [00:00<00:00, 920.21it/s]\n"
     ]
    }
   ],
   "source": [
    "loader = GenericLoader.from_filesystem(\n",
    "    path=repo_path,\n",
    "    glob=\"**/*.py\",\n",
    "    parser = LanguageParser(language=Language.PYTHON),\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c88a8521",
   "metadata": {},
   "outputs": [],
   "source": [
    "RecursiveCharacterTextSplitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language = Language.PYTHON,\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "text_chunks = RecursiveCharacterTextSplitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84c2a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents= text_chunks, embedding=OpenAIEmbeddings())\n",
    "# Save to local folder\n",
    "vectorstore.save_local(\"/Users/yogeshagrawal/Desktop/Gen AI/27.Code_Explainer/faiss_index\")\n",
    "\n",
    "# faiss_store = FAISS.load_local(\"faiss_index\", embeddings=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5045d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type='mmr', search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8724c00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30f5b84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7l/dw4qxb8d2qb48m997dzvxtsc0000gn/T/ipykernel_74735/2314359002.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm = llm, memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryMemory(llm = llm, memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15ad3c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "651032ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is download_hugging_face_embeddings function ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8a55ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7l/dw4qxb8d2qb48m997dzvxtsc0000gn/T/ipykernel_74735/79176006.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "def download_hugging_face_embeddings():\n",
      "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')  #this model return 384 dimensions\n",
      "    return embeddings\n",
      "\n",
      "from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings\n",
      "from pinecone.grpc import PineconeGRPC as Pinecone\n",
      "from pinecone import ServerlessSpec\n",
      "from langchain_pinecone import PineconeVectorStore\n",
      "from dotenv import load_dotenv\n",
      "import os\n",
      "\n",
      "\n",
      "load_dotenv()\n",
      "\n",
      "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\n",
      "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
      "\n",
      "system_prompt = (\n",
      "    \"You are an assistant for question-answering tasks. \"\n",
      "    \"Use the following pieces of retrieved context to answer \"\n",
      "    \"the question. If you don't know the answer, say that you \"\n",
      "    \"don't know. Use three sentences maximum and keep the \"\n",
      "    \"answer concise.\"\n",
      "    \"\\n\\n\"\n",
      "    \"{context}\"\n",
      ")\n",
      "\n",
      "def index():\n",
      "    return render_template('chat.html')\n",
      "\n",
      "from setuptools import find_packages, setup\n",
      "\n",
      "setup(\n",
      "    name = 'Generative AI Project',\n",
      "    version= '0.0.0',\n",
      "    author= 'Bappy Ahmed',\n",
      "    author_email= 'entbappy73@gmail.com',\n",
      "    packages= find_packages(),\n",
      "    install_requires = []\n",
      "\n",
      ")\n",
      "Human: what is download_hugging_face_embeddings function ?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The `download_hugging_face_embeddings` function is a function that downloads a pre-trained model from the Hugging Face Transformers library, specifically the \"sentence-transformers/all-MiniLM-L6-v2\" model, which returns embeddings of 384 dimensions for text data.\n"
     ]
    }
   ],
   "source": [
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef06721a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa1242e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b033e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e29182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6ad2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c40303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3315cc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7aeb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042409e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22674acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082a79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a59c508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9d282f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f3f47e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
