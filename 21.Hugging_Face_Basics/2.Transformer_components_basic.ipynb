{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09c44282",
   "metadata": {},
   "source": [
    "##1. Using very basic way of calling each component\n",
    "1. Assign tokenizer\n",
    "2. Assign Pre-trained Model\n",
    "3. Run Tokenizer to encode input prompt into embedding ids\n",
    "4. Pass it to Transformer blocks ( self-attention & Feed Forword NN ) & generate Intermediate output\n",
    "5. Pass it to LM head model to get probability value\n",
    "6. Use numpy to extract the next text \n",
    "7. pass tokenizer to get the decoded value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc572f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yogeshagrawal/Desktop/Gen AI/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:40<00:00, 20.37s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "\n",
    "# Prompt\n",
    "# Prompt (formatted for Phi-3)\n",
    "prompt = \"<|user|>\\nWhat is the capital of France?\\n<|assistant|>\\n\"\n",
    "\n",
    "# Load the AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", trust_remote_code=True)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                                        \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "                                        attn_implementation=\"eager\",  # <-- disables FlashAttention attempt \n",
    "                                        trust_remote_code=True\n",
    "        )\n",
    "model.eval()\n",
    "\n",
    "# Tokenize the input prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# Self-attention layer output - Get the output of the model before the lm_head\n",
    "\n",
    "model_output = model.model(input_ids)  # hidden states\n",
    "hidden_states = model_output.last_hidden_state  # shape: [1, seq_len, hidden_dim]\n",
    "\n",
    "# Passing output to final linear layer and softmax to get probability value - Get the output of the lm_head\n",
    "lm_head_output = model.lm_head(hidden_states)  # shape: [1, seq_len, vocab_size]\n",
    "\n",
    "# Get the top predicted token ID for the last position\n",
    "token_id = lm_head_output[0, -1].argmax(-1)\n",
    "\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# last_token_logits = lm_head_output[0, -1]\n",
    "# probs = F.softmax(last_token_logits, dim=-1)\n",
    "# token_id = torch.argmax(probs).item()\n",
    "# # Decode the token ID to readable text\n",
    "# predicted_token = tokenizer.decode(token_id)\n",
    "# print(\"Predicted token:\", predicted_token)\n",
    "\n",
    "# Decode the token ID to readable text\n",
    "predicted_token = tokenizer.decode(token_id.item())\n",
    "print(\"Predicted token:\", predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2625638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yogeshagrawal/Desktop/Gen AI/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:42<00:00, 21.28s/it]\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: The\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # Prompt (formatted for Phi-3)\n",
    "# prompt = \"<|user|>\\nWhat is the capital of France?\\n<|assistant|>\\n\"\n",
    "\n",
    "# # Load the AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", trust_remote_code=True)\n",
    "\n",
    "# # Load the model\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "#                                             attn_implementation=\"eager\",  # <-- disables FlashAttention attempt \n",
    "#                                             trust_remote_code=True\n",
    "#                                     )\n",
    "# model.eval()\n",
    "\n",
    "# # Tokenize the input prompt\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# # Self-attention layer output - Get the output of the model before the lm_head\n",
    "# with torch.no_grad():\n",
    "#     model_output = model.model(input_ids)  # hidden states\n",
    "#     hidden_states = model_output.last_hidden_state  # shape: [1, seq_len, hidden_dim]\n",
    "\n",
    "# # Passing output to final linear layer and softmax to get probability value - Get the output of the lm_head\n",
    "# lm_head_output = model.lm_head(hidden_states)  # shape: [1, seq_len, vocab_size]\n",
    "\n",
    "# # Get the top predicted token ID for the last position\n",
    "# last_token_logits = lm_head_output[0, -1]\n",
    "# probs = F.softmax(last_token_logits, dim=-1)\n",
    "# token_id = torch.argmax(probs).item()\n",
    "\n",
    "# # Decode the token ID to readable text\n",
    "# predicted_token = tokenizer.decode(token_id)\n",
    "# print(\"Predicted token:\", predicted_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
